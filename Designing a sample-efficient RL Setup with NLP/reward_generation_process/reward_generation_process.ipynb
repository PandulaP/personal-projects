{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-26T15:54:09.759333Z",
     "start_time": "2020-09-26T15:54:09.748701Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install -U sentence-transformers\n",
    "\n",
    "def intermediate_rewards_function(three_frames, language_instruction):\n",
    "    \"\"\"This function takes 3 (consecutive) images/game frames and a language instruction and evaluates \n",
    "       if the agent's behaviour in the frames agrees with the given language instruction.\n",
    "       \n",
    "       It uses the the previously trained LIERN model and outputs a reward of `1' if agent's behaviour\n",
    "       aligns with the given instruction (and `0' otherwise).\"\"\"\n",
    "    \n",
    "    \n",
    "    ### IMPORT NECESSARY PACKAGES ###\n",
    "\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torchvision.models as models\n",
    "    import torchvision.transforms as transforms\n",
    "    from torch.autograd import Variable\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    \n",
    "    ### CONVERTING IMAGES TO EMBEDDING VECTOR ###\n",
    "    \n",
    "    img_model = models.resnet50(pretrained=True)\n",
    "    layer = img_model._modules.get('avgpool')\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    img_model.eval()\n",
    "\n",
    "    def get_img_embeddings(image_array):\n",
    "        \"\"\"gets 3 input frames/images and converts them to a single embedding vector\"\"\"\n",
    "\n",
    "        temp_imgs_l = []\n",
    "\n",
    "        for image in image_array:\n",
    "            temp_img = Image.fromarray(image).convert('RGB')\n",
    "            temp_img = normalize(to_tensor(scaler(temp_img)))\n",
    "            temp_imgs_l.append(temp_img)\n",
    "\n",
    "        temp_imgs_l = torch.cat(temp_imgs_l)\n",
    "        temp_imgs_l = temp_imgs_l.reshape((3,3,224,224))\n",
    "\n",
    "        # create PyTorch Var. w/ pre-processed image\n",
    "        t_img = Variable(temp_imgs_l)\n",
    "\n",
    "        # create an empty vector to hold the embeddings vector\n",
    "        img_embedding = torch.zeros(2048*3)\n",
    "\n",
    "        # function to copy the output of the layer\n",
    "        def copy_data(m, i, o):\n",
    "            img_embedding.copy_(o.data.reshape(2048*3))\n",
    "\n",
    "        # attach that function to the `avgpool` layer\n",
    "        h = layer.register_forward_hook(copy_data)\n",
    "\n",
    "        # run the model on the image\n",
    "        img_model(t_img)\n",
    "\n",
    "        # remove the copy function from the layer\n",
    "        h.remove()\n",
    "\n",
    "        # return the feature vector\n",
    "        return img_embedding\n",
    "\n",
    "    \n",
    "    ### CONVERTING LANGUAGE INSTRUCTION TO EMBEDDING VECTOR ###\n",
    "    \n",
    "    sentence_model = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n",
    "\n",
    "    def get_sentence_embeddings(sentence_model, language_instruct):\n",
    "\n",
    "        sentc_embedding = sentence_model.encode(language_instruct)\n",
    "\n",
    "        return sentc_embedding.reshape(-1).tolist()\n",
    "    \n",
    "    \n",
    "    ### GENERATING THE COMBINED EMBEDDINGS VECTOR (INPUT TO CLASSIFICATION MODEL) ###\n",
    "    \n",
    "    def get_combined_embeddings(img_embed, sentence_embed):\n",
    "\n",
    "        return np.concatenate([img_embed, sentence_embed]).tolist()\n",
    "    \n",
    "    \n",
    "    ### GENERATE INTERMEDIATE REWARDS USING SAVED MODEL ###\n",
    "    \n",
    "    class Classification_Net(nn.Module):\n",
    "\n",
    "        def __init__(self, n_features):\n",
    "            super(Classification_Net, self).__init__()\n",
    "            self.fc1 = nn.Linear(n_features, int(7168/2))\n",
    "            self.fc2 = nn.Linear(int(7168/2), int(7168/10))\n",
    "            self.fc3 = nn.Linear(int(7168/10), int(7168/100))\n",
    "            self.fc4 = nn.Linear(int(7168/100), 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = F.relu(self.fc3(x))\n",
    "            return torch.sigmoid(self.fc4(x))\n",
    "\n",
    "    # load the previously trained model parameters\n",
    "    classify_net = torch.load('classify_model.pth')\n",
    "\n",
    "    def get_probability(model, combined_embed):\n",
    "        \n",
    "        classify_net = model\n",
    "        embed_input = torch.from_numpy(np.array(combined_embed)).float()\n",
    "        y_pred - classify_net(embed_input)\n",
    "        return y_pred\n",
    "\n",
    "    def generate_interim_reward(predicted_prob):\n",
    "\n",
    "        if predicted_prob > .5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    img_embeddings = get_img_embeddings(three_frames)\n",
    "    sentence_embeddings =  get_sentence_embeddings(sentence_model, language_instruction)\n",
    "    combined_embeddings = get_combined_embeddings(img_embeddings,sentence_embeddings)\n",
    "    probability = get_probability(classify_net,combined_embeddings)\n",
    "    interim_reward = generate_interim_reward(probability)\n",
    "    \n",
    "    return interim_reward"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
