{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Generate training (chart) data - PARELLELIZE EXPERIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf\n",
    "import multiprocessing as mp\n",
    "from PIL import Image\n",
    "import os\n",
    "import tqdm\n",
    "from datetime import datetime\n",
    "import create_chart\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instruments = con.get_instruments()\n",
    "curr_pairs = ['EUR/USD',\n",
    "             'GBP/USD',\n",
    "             'USD/CHF',\n",
    "             'AUD/USD',\n",
    "             'USD/CAD',\n",
    "             'NZD/USD',\n",
    "             'EUR/CHF',\n",
    "             'EUR/GBP',\n",
    "             'EUR/AUD',\n",
    "             'EUR/CAD']\n",
    "\n",
    "curr_pair = curr_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "Curr. Pair: EUR-USD\n",
      "\n",
      "Start-time: 2020-06-25 07:15:00,\n",
      "End-time: 2020-11-19 13:45:00,\n",
      "\n",
      "Signal-Breakdown\n",
      "      signal\n",
      "HOLD   10225\n",
      "BUY       10\n",
      "SELL       7\n",
      "\n",
      "Dataset-size:(10242, 11)\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "# extract saved signal and price data | merge \n",
    "folder_path    = f'./data/fx_data/{curr_pair[:3]}_{curr_pair[4:]}/'\n",
    "fx_signal_data = pd.read_parquet(folder_path+f'fx_data_{curr_pair[:3]}_{curr_pair[4:]}_w_sig.parquet')\n",
    "fx_data        = pd.read_parquet(folder_path+f'fx_data_{curr_pair[:3]}_{curr_pair[4:]}.parquet')\n",
    "fx_data.set_index('date',inplace = True)\n",
    "fx_final = fx_data.merge(right = fx_signal_data\n",
    "                         , left_index=True\n",
    "                         , right_index=True\n",
    "                         , how = 'inner')\n",
    "\n",
    "# print status\n",
    "print('--------------------------\\n'\\\n",
    "      f'Curr. Pair: {curr_pair[:3]}-{curr_pair[4:]}\\n\\n' \\\n",
    "      f'Start-time: {fx_final.index.min()},\\nEnd-time: {fx_final.index.max()},\\n\\n' \\\n",
    "      f'Signal-Breakdown\\n{pd.DataFrame(fx_final.signal.value_counts())}\\n\\n' \\\n",
    "      f'Dataset-size:{fx_final.shape}\\n'\\\n",
    "       '--------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single `open', `close', `high', `low' metrics with bid/ask prices \n",
    "fx_final.loc[:,'open']  = (fx_final.loc[:,'bidopen']  + fx_final.loc[:,'askopen'])/2 \n",
    "fx_final.loc[:,'close'] = (fx_final.loc[:,'bidclose'] + fx_final.loc[:,'askclose'])/2 \n",
    "fx_final.loc[:,'high']  = (fx_final.loc[:,'bidhigh']  + fx_final.loc[:,'askhigh'])/2 \n",
    "fx_final.loc[:,'low']   = (fx_final.loc[:,'bidlow']   + fx_final.loc[:,'asklow'])/2 \n",
    "fx_final.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out only the required metrics / structure the dataframe\n",
    "fx_final = fx_final.loc[:,['open','close','high','low','tickqty', 'signal', 'signal_count']]\n",
    "fx_final.columns = ['open','close','high','low','volume', 'signal', 'signal_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basic moving-average indicators (50 EMA 200 EMA)\n",
    "fx_final['ewm_50_m'] = fx_final['close'].ewm(span=50\n",
    "                                           , min_periods=0\n",
    "                                           , adjust=False\n",
    "                                           , ignore_na=False).mean()\n",
    "\n",
    "fx_final['ewm_50_h'] = fx_final['high'].ewm(span=50\n",
    "                                           , min_periods=0\n",
    "                                           , adjust=False\n",
    "                                           , ignore_na=False).mean()\n",
    "\n",
    "fx_final['ewm_50_l'] = fx_final['low'].ewm(span=50\n",
    "                                           , min_periods=0\n",
    "                                           , adjust=False\n",
    "                                           , ignore_na=False).mean()\n",
    "\n",
    "fx_final['ewm_200'] = fx_final['close'].ewm(span=200\n",
    "                                           , min_periods=0\n",
    "                                           , adjust=False\n",
    "                                           , ignore_na=False).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of candle-stick history to consider for each prediction\n",
    "look_back_time = 40\n",
    "\n",
    "try:\n",
    "    # get the image labels in the target folder\n",
    "    file_name_li = os.listdir(folder_path+'train_imgs/')\n",
    "\n",
    "    # extract the time labels from files\n",
    "    file_time_li = [datetime.strptime(file_name[8:24], '%Y-%m-%d_%H-%M') for file_name in file_name_li]\n",
    "\n",
    "    # get the most recent time\n",
    "    max_time_existing = max(file_time_li)\n",
    "    \n",
    "    # filter time index for new data (excluding already existing data)\n",
    "    fx_final_date_filtered = fx_final.index[fx_final.index>max_time_existing]\n",
    "    \n",
    "    # create incremental image chunks with 30 images (each chunk include 30 candles)\n",
    "    data_idx_chuncks = [i for i in zip(fx_final_date_filtered.to_list(), fx_final_date_filtered.to_list()[look_back_time:])]\n",
    "    \n",
    "except:\n",
    "    # create incremental image chunks with 30 images (each chunk include 30 candles)\n",
    "    data_idx_chuncks = [i for i in zip(fx_final.index.to_list(), fx_final.index.to_list()[look_back_time:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5046/5046 [00:01<00:00, 2607.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# structure the data for parallel processing\n",
    "\n",
    "# add curr. pair information to dataframe\n",
    "fx_final.loc[:,\"curr_1\"] = curr_pair[:3]\n",
    "fx_final.loc[:,\"curr_2\"] = curr_pair[4:]\n",
    "\n",
    "# add folder_path to dataframe\n",
    "fx_final.loc[:,\"f_path\"] = folder_path\n",
    "\n",
    "# create data chunks based on time indexes\n",
    "df_chuncks = []\n",
    "\n",
    "for data_chunk in tqdm.tqdm(data_idx_chuncks):\n",
    "    df_chuncks.append(fx_final.loc[data_chunk[0]:data_chunk[1]])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### FUNCTION PORTED TO `create_chart.py' TO MITIGATE THE ISSUE WITH MULTIPROCESSING.POOL ####\n",
    "\n",
    "def create_chart_parellel(df_chunk):\n",
    "    \"\"\"Function to parallalize the training data (chart) creation\"\"\"\n",
    "    # filtered df to generate images from (30 candles history for each signal)\n",
    "    temp_df = df_chunk\n",
    "    \n",
    "    # sort values by date index\n",
    "    temp_df.sort_index(inplace=True,ascending=True)\n",
    "    \n",
    "    # Signal label\n",
    "    label = temp_df.signal.values[-1]\n",
    "\n",
    "    # image pre-fix time tag\n",
    "    date_start = temp_df.index.min().strftime('%Y-%m-%d_%H-%M')    \n",
    "    \n",
    "    # image post-fix time tag\n",
    "    date_predict = temp_df.index.max().strftime('%Y-%m-%d_%H-%M')\n",
    "    \n",
    "    # extract curr. pair labels\n",
    "    curr_1 = temp_df.curr_1.unique()[0]\n",
    "    curr_2 = temp_df.curr_2.unique()[0]\n",
    "    \n",
    "    # path to store image\n",
    "    image_path = folder_path+'train_imgs/'+f'{curr_1}_{curr_2}_{date_start}_{date_predict}_{label}.jpg'\n",
    "    \n",
    "    apds = [mpf.make_addplot(temp_df['ewm_50_h']\n",
    "                             #, linestyle ='dashdot'\n",
    "                             , color = 'blue'\n",
    "                             , alpha = 0.2),\n",
    "            mpf.make_addplot(temp_df['ewm_50_m']\n",
    "                             #, linestyle ='dashdot'\n",
    "                             , color = 'darkblue'\n",
    "                             , alpha = 0.7),\n",
    "            mpf.make_addplot(temp_df['ewm_50_l']\n",
    "                             #, linestyle ='dashdot'\n",
    "                             , color = 'blue'\n",
    "                             , alpha = 0.2),\n",
    "            mpf.make_addplot(temp_df['ewm_200']\n",
    "                             #, linestyle ='line'\n",
    "                             , color='red')\n",
    "           ]\n",
    "\n",
    "    s  = mpf.make_mpf_style(base_mpf_style='charles'\n",
    "                            , gridstyle = 'dashed')\n",
    "\n",
    "    export_image = mpf.plot(temp_df\n",
    "                            , type = 'candle'\n",
    "                            , style = s\n",
    "                            , volume = True\n",
    "                            , ylabel=''\n",
    "                            , ylabel_lower=''\n",
    "                            , figratio = (12,8)\n",
    "                            , tight_layout = True\n",
    "                            , addplot = apds\n",
    "                            , panel_ratios = (6,1)\n",
    "                            , fill_between = dict(y1 = temp_df['ewm_50_h'].values\n",
    "                                                  , y2 = temp_df['ewm_50_l'].values\n",
    "                                                  , alpha=0.2\n",
    "                                                  , color='b')\n",
    "                            , scale_width_adjustment = dict(volume=0.7\n",
    "                                                            , candle=1.2\n",
    "                                                            , lines=0.3)\n",
    "                            , savefig=dict(fname=image_path\n",
    "                                           #, dpi=100\n",
    "                                           , pad_inches=0.25))\n",
    "    \n",
    "    return date_predict # return the date that the signal is predicted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder to store images\n",
    "if not os.path.exists(folder_path+'train_imgs'):\n",
    "     os.makedirs(folder_path+'train_imgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 5046/5046 [19:13<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# parallelize\n",
    "# with mp.Pool(processes=5) as pool:\n",
    "#       predicted_dates = list()\n",
    "predicted_dates = []\n",
    "\n",
    "Pool = mp.Pool(processes=5)\n",
    "for _ in tqdm.tqdm(Pool.imap(create_chart.create_chart_parellel, df_chuncks), total=len(df_chuncks)):\n",
    "    predicted_dates.append(_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read old log datafile\n",
    "with open(folder_path+curr_pair[:3]+'_'+curr_pair[4:]+\"_chart_log.txt\") as json_file:\n",
    "    log_data = json.load(json_file)\n",
    "\n",
    "# add new log data\n",
    "log_data[datetime.now().strftime('%Y-%m-%d %H:%M:%S')] = predicted_dates\n",
    "\n",
    "# overwite the log datafile\n",
    "with open(folder_path+curr_pair[:3]+'_'+curr_pair[4:]+\"_chart_log.txt\", 'w') as outfile:\n",
    "    json.dump(log_data, outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
